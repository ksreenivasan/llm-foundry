{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_list = [\n",
    "    'debug-mpt-7b-base-gsm8k-ft',\n",
    "    'debug-mpt-7b-base-metamathqa-ft',\n",
    "    'debug-mpt-7b-base-metamathqa-bs48-ft',\n",
    "    'debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft',\n",
    "]\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "num_few_shot = 8\n",
    "\n",
    "def pretty_print(df, split='train', idx=None):\n",
    "    IS_CORRECT=False\n",
    "    if idx == None:\n",
    "        print(\"Index not passed, picking a random row.\")\n",
    "        idx = np.random.randint(0, len(df))\n",
    "    print(f\"Showing {split} example at index {idx}\")\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"Prompt:\\n{row['prompt']}\\n\\n\")\n",
    "    print(f\"Correct Answer:\\n{bcolors.OKGREEN}{row['answer']}{bcolors.ENDC}\\n\\n\")\n",
    "    actual_answer = row['answer'].split('####')[-1].strip()\n",
    "    if actual_answer in row['model_generation']:\n",
    "        pretty_generation = row['model_generation'].replace(actual_answer, f\"{bcolors.UNDERLINE}{bcolors.OKGREEN}{actual_answer}{bcolors.ENDC}\")\n",
    "        print(f\"Generated Answer:\\n{pretty_generation}\\n\\n\")\n",
    "        IS_CORRECT=True\n",
    "    else:\n",
    "        # print generated answers in red\n",
    "        print(f\"Generated Answer:\\n{bcolors.FAIL}{row['model_generation']}{bcolors.ENDC}\\n\\n\")\n",
    "    \n",
    "    if IS_CORRECT:\n",
    "        print(f\"{bcolors.OKGREEN}It actually got it right!{bcolors.ENDC}\")\n",
    "    else:\n",
    "        print(f\"{bcolors.FAIL}Boo. Got it wrong.{bcolors.ENDC}\")\n",
    "    return IS_CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Model debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft with 8 shot generations.\n"
     ]
    }
   ],
   "source": [
    "model_idx = 3\n",
    "num_few_shot = 8\n",
    "\n",
    "def get_model_generations(model_idx=0, num_few_shot=0):\n",
    "    filename = f'/mnt/workdisk/kartik/llm-foundry/{model_list[model_idx]}_{num_few_shot}_shot_gsm8k_generations.csv'\n",
    "    print(f\"Inspecting Model {model_list[model_idx]} with {num_few_shot} shot generations.\")\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "df = get_model_generations(model_idx=model_idx, num_few_shot=num_few_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'm fairly convinced that the metamathqa model is the best one, but doesn't know when to shut up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_stats(df):\n",
    "    # check metmath accuracy\n",
    "    num_correct = 0\n",
    "    num_maybe_correct = 0\n",
    "    num_rambles = 0\n",
    "    num_wrong = 0\n",
    "    for idx in range(len(df)):\n",
    "        row = df.iloc[idx]\n",
    "        actual_answer = row['answer'].split('####')[-1].strip()\n",
    "        if f\"#### {actual_answer}\" in row['model_generation']:\n",
    "            num_correct += 1\n",
    "            if len(row['model_generation'].split(f'#### {actual_answer}')[-1]) > 30:\n",
    "                num_rambles += 1\n",
    "        elif actual_answer in row['model_generation']:\n",
    "            # looser check. might be a false positive\n",
    "            num_maybe_correct += 1\n",
    "        else:\n",
    "            num_wrong += 1\n",
    "    print(f\"Model gets {num_correct} correct out of {len(df)}.\")\n",
    "    print(f\"Model rambles {num_rambles} times after getting the right answer.\")\n",
    "    print(f\"Model gets {num_maybe_correct} somewhat correct out of {len(df)}.\")\n",
    "    print(f\"Model gets {num_wrong} wrong out of {len(df)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Model debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft with 8 shot generations.\n"
     ]
    }
   ],
   "source": [
    "df = get_model_generations(model_idx=3, num_few_shot=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft on train set\n",
      "Model gets 61 correct out of 100.\n",
      "Model rambles 52 times after getting the right answer.\n",
      "Model gets 10 somewhat correct out of 100.\n",
      "Model gets 29 wrong out of 100.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating {model_list[3]} on train set\")\n",
    "get_accuracy_stats(df[df['split'] == 'train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft on test set\n",
      "Model gets 36 correct out of 100.\n",
      "Model rambles 33 times after getting the right answer.\n",
      "Model gets 18 somewhat correct out of 100.\n",
      "Model gets 46 wrong out of 100.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating {model_list[3]} on test set\")\n",
    "get_accuracy_stats(df[df['split'] == 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Model debug-mpt-7b-base-gsm8k-ft with 8 shot generations.\n",
      "---------------------------------------------------------------\n",
      "Evaluating debug-mpt-7b-base-gsm8k-ft on train set\n",
      "Model gets 6 correct out of 100.\n",
      "Model rambles 3 times after getting the right answer.\n",
      "Model gets 24 somewhat correct out of 100.\n",
      "Model gets 70 wrong out of 100.\n",
      "Evaluating debug-mpt-7b-base-gsm8k-ft on test set\n",
      "Model gets 15 correct out of 100.\n",
      "Model rambles 7 times after getting the right answer.\n",
      "Model gets 22 somewhat correct out of 100.\n",
      "Model gets 63 wrong out of 100.\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\n",
      "Inspecting Model debug-mpt-7b-base-metamathqa-ft with 8 shot generations.\n",
      "---------------------------------------------------------------\n",
      "Evaluating debug-mpt-7b-base-metamathqa-ft on train set\n",
      "Model gets 42 correct out of 100.\n",
      "Model rambles 0 times after getting the right answer.\n",
      "Model gets 14 somewhat correct out of 100.\n",
      "Model gets 44 wrong out of 100.\n",
      "Evaluating debug-mpt-7b-base-metamathqa-ft on test set\n",
      "Model gets 33 correct out of 100.\n",
      "Model rambles 0 times after getting the right answer.\n",
      "Model gets 11 somewhat correct out of 100.\n",
      "Model gets 56 wrong out of 100.\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\n",
      "Inspecting Model debug-mpt-7b-base-metamathqa-bs48-ft with 8 shot generations.\n",
      "---------------------------------------------------------------\n",
      "Evaluating debug-mpt-7b-base-metamathqa-bs48-ft on train set\n",
      "Model gets 65 correct out of 100.\n",
      "Model rambles 0 times after getting the right answer.\n",
      "Model gets 5 somewhat correct out of 100.\n",
      "Model gets 30 wrong out of 100.\n",
      "Evaluating debug-mpt-7b-base-metamathqa-bs48-ft on test set\n",
      "Model gets 35 correct out of 100.\n",
      "Model rambles 0 times after getting the right answer.\n",
      "Model gets 13 somewhat correct out of 100.\n",
      "Model gets 52 wrong out of 100.\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\n",
      "Inspecting Model debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft with 8 shot generations.\n",
      "---------------------------------------------------------------\n",
      "Evaluating debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft on train set\n",
      "Model gets 61 correct out of 100.\n",
      "Model rambles 52 times after getting the right answer.\n",
      "Model gets 10 somewhat correct out of 100.\n",
      "Model gets 29 wrong out of 100.\n",
      "Evaluating debug-mpt-7b-base-metamathqa-fixed-packing-ratio-ft on test set\n",
      "Model gets 36 correct out of 100.\n",
      "Model rambles 33 times after getting the right answer.\n",
      "Model gets 18 somewhat correct out of 100.\n",
      "Model gets 46 wrong out of 100.\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_idx in range(len(model_list)):\n",
    "    df = get_model_generations(model_idx=model_idx, num_few_shot=8)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(f\"Evaluating {model_list[model_idx]} on train set\")\n",
    "    get_accuracy_stats(df[df['split'] == 'train'])\n",
    "\n",
    "    print(f\"Evaluating {model_list[model_idx]} on test set\")\n",
    "    get_accuracy_stats(df[df['split'] == 'test'])\n",
    "    print(\"---------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: The metamathQA model actually is the best model, despite the formatting. It just needs to learn how to stop talking after spitting out the answer. Also, incorrect packing ratio seems to be the reason why it was rambling. Once that gets changed to auto, changing the batch size/format affects accuracy but doesn't change the rambling behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-llmfoundry-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
